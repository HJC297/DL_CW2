{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzzPw7rEZ-OX"
   },
   "source": [
    "# Coursework 2: Generative Models\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Please submit on CATe two zip files:\n",
    "\n",
    "*CW2.zip* containing the following:\n",
    "1. A version of this notebook containing your answers. Write your answers in the cells below each question. **Please deliver the notebook including the outputs of the cells below.**\n",
    "2. Your trained VAE model as *VAE_model.pth*\n",
    "\n",
    "*GAN.zip* containing your trained Generator and Discriminator: *DCGAN_model_D.pth and DCGAN_model_G.pth*\n",
    "\n",
    "Please avoid using markdown headings (# ## etc.) as these will affect the ToC. Instead use html headings if you want emphasis.\n",
    "\n",
    "Similarly to the previous coursework, we recommend that you use Google Colaboratory in order to train the required networks.\n",
    "\n",
    "TAs will run a testing cell (at the end of this notebook), so you are required to copy your transform and denorm functions to a cell near the bottom of the document (it is demarkated).\n",
    "\n",
    "<font color=\"blue\">**The deadline for submission is 19:00, Thursday 19th February, 2021** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oqY55OLpxDm"
   },
   "source": [
    "## Setting up working environment\n",
    "\n",
    "For this coursework you, will need to train a large network, therefore we recommend you work with Google Colaboratory, which provides free GPU time. You will need a Google account to do so.\n",
    "\n",
    "Please log in to your account and go to the following page: https://colab.research.google.com. Then upload this notebook.\n",
    "\n",
    "For GPU support, go to \"Edit\" -> \"Notebook Settings\", and select \"Hardware accelerator\" as \"GPU\".\n",
    "\n",
    "You will need to install pytorch and import some utilities by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FJg7ozC_q3HF"
   },
   "outputs": [],
   "source": [
    "#!pip install -q torch torchvision altair matplotlib pandas\n",
    "#!git clone -q https://github.com/afspies/icl_dl_cw2_utils\n",
    "from icl_dl_cw2_utils.utils.plotting import plot_tsne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezLSfB6IqAzK"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "For this coursework, you are asked to implement two commonly used generative models:\n",
    "1. A **Variational Autoencoder (VAE)**\n",
    "2. A **Deep Convolutional Generative Adversarial Network (DCGAN)**\n",
    "\n",
    "For the first part you will the MNIST dataset https://en.wikipedia.org/wiki/MNIST_database and for the second the CIFAR-10 (https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "Each part is worth 50 points. \n",
    "\n",
    "The emphasis of both parts lies in understanding how the models behave and learn, however, some points will be available for getting good results with your GAN (though you should not spend too long on this)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75mICbvzqQyx"
   },
   "source": [
    "# Part 1 - Variational Autoencoder\n",
    "\n",
    "## Part 1.1 (25 points)\n",
    "**Your Task:**\n",
    "\n",
    "a. Implement the VAE architecture with accompanying hyperparameters. Experiment with Feedforward and Convolutional Layers to see which gives better results.\n",
    "\n",
    "b. Design an appropriate loss function and train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ym5l5RmmJtLw",
    "outputId": "75c1aad7-1143-4175-f694-ad0860c77890"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, sampler\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show(img):\n",
    "    npimg = img.cpu().numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "\n",
    "#if not os.path.exists('/content/drive/MyDrive/icl_dl_cw2/CW_VAE/'):\n",
    "    #os.makedirs('/content/drive/MyDrive/icl_dl_cw2/CW_VAE/')\n",
    "\n",
    "# We set a random seed to ensure that your results are reproducible.\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(0)\n",
    "\n",
    "GPU = True # Choose whether to use GPU\n",
    "if GPU:\n",
    "    device = torch.device(\"cuda\"  if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f'Using {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqT7sdGzJtLy"
   },
   "source": [
    "---\n",
    "## Part 1.1a: Implement VAE (25 Points)\n",
    "###Hyper-parameter selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZVPM6pgqJtLz"
   },
   "outputs": [],
   "source": [
    "# Necessary Hyperparameters \n",
    "num_epochs = 1\n",
    "learning_rate = 0.1 \n",
    "batch_size = 32\n",
    "latent_dim = 100 # Choose a value for the size of the latent space\n",
    "\n",
    "# Additional Hyperparameters \n",
    "\n",
    "\n",
    "# (Optionally) Modify transformations on input\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# (Optionally) Modify the network's output for visualizing your images\n",
    "def denorm(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iN5aL7sdJtL2"
   },
   "source": [
    "### Data loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VOKdAZa2JtL3"
   },
   "outputs": [],
   "source": [
    "train_dat = datasets.MNIST(\n",
    "    \"data/\", train=True, download=True, transform=transform\n",
    ")\n",
    "test_dat = datasets.MNIST(\"data/\", train=False, transform=transform)\n",
    "\n",
    "loader_train = DataLoader(train_dat, batch_size, shuffle=True)\n",
    "loader_test = DataLoader(test_dat, batch_size, shuffle=False)\n",
    "\n",
    "# Don't change \n",
    "sample_inputs, _ = next(iter(loader_test))\n",
    "fixed_input = sample_inputs[:32, :, :, :]\n",
    "save_image(fixed_input, '/content/drive/MyDrive/icl_dl_cw2/CW_VAE/image_original.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LiQDXD24JtL7"
   },
   "source": [
    "### Model Definition\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://blog.bayeslabs.co/assets/img/vae-gaussian.png\" style=\"width:60%\">\n",
    "  <figcaption>\n",
    "    Fig.1 - VAE Diagram (with a Guassian prior), taken from <a href=\"https://blog.bayeslabs.co/2019/06/04/All-you-need-to-know-about-Vae.html\">1</a>.\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "You will need to define:\n",
    "* The hyperparameters\n",
    "* The constructor\n",
    "* encode\n",
    "* reparametrize\n",
    "* decode\n",
    "* forward\n",
    "\n",
    "\n",
    "\n",
    "Hints:\n",
    "- It is common practice to encode the log of the variance, rather than the variance\n",
    "- You might try using BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28])\n",
      "torch.Size([32, 16, 28, 28])\n",
      "torch.Size([32, 32, 14, 14])\n",
      "torch.Size([32, 64, 7, 7])\n",
      "\n",
      "torch.Size([32, 3136])\n",
      "\n",
      "torch.Size([32, 100])\n",
      "\n",
      "torch.Size([32, 3136])\n",
      "\n",
      "torch.Size([32, 3136, 1, 1])\n",
      "torch.Size([32, 32, 5, 5])\n",
      "torch.Size([32, 16, 14, 14])\n",
      "torch.Size([32, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(32,1,28,28)\n",
    "print(x.shape)\n",
    "\n",
    "a = nn.Sequential(nn.Conv2d(1,16,kernel_size=3,stride=1,padding=1),nn.ReLU())\n",
    "x = a(x)\n",
    "print(x.shape)\n",
    "\n",
    "b = nn.Sequential(nn.Conv2d(16,32,kernel_size=4,stride=2,padding=1),nn.ReLU())\n",
    "x = b(x)\n",
    "print(x.shape)\n",
    "\n",
    "c = nn.Sequential(nn.Conv2d(32,64,kernel_size=4,stride=2,padding=1),nn.ReLU())\n",
    "x = c(x)\n",
    "print(x.shape)\n",
    "print('')\n",
    "\n",
    "x = x.view(x.size(0),-1)\n",
    "print(x.shape)\n",
    "print('')\n",
    "\n",
    "d = nn.Linear(64*7*7,100)\n",
    "x = d(x)\n",
    "print(x.shape)\n",
    "print('')\n",
    "\n",
    "e = nn.Linear(100, 64*7*7)\n",
    "x = e(x)\n",
    "print(x.shape)\n",
    "print('')\n",
    "\n",
    "x = x.view(x.size(0),64*7*7,1,1)\n",
    "print(x.shape)\n",
    "\n",
    "f = nn.ConvTranspose2d(64*7*7, 32, kernel_size=5,stride=2)\n",
    "x = f(x)\n",
    "print(x.shape)\n",
    "\n",
    "g = nn.ConvTranspose2d(32,16,kernel_size=6,stride=2)\n",
    "x = g(x)\n",
    "print(x.shape)\n",
    "\n",
    "h = nn.ConvTranspose2d(16,1,kernel_size=2,stride=2)\n",
    "x = h(x)\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "wDlll3BUJtL8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters is: 3512697\n",
      "VAE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (fc1): Linear(in_features=3136, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=3136, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=3136, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(3136, 32, kernel_size=(5, 5), stride=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): ConvTranspose2d(32, 16, kernel_size=(6, 6), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): ConvTranspose2d(16, 1, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (5): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# *CODE FOR PART 1.1a IN THIS CELL*\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(nn.Conv2d(1,16,kernel_size=3,stride=1,padding=1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Conv2d(16,32,kernel_size=4,stride=2,padding=1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Conv2d(32,64,kernel_size=4,stride=2,padding=1),\n",
    "                                     nn.ReLU()\n",
    "                                    )\n",
    "        \n",
    "        self.fc1 = nn.Linear(64*7*7,latent_dim)\n",
    "        self.fc2 = nn.Linear(64*7*7,latent_dim)\n",
    "        \n",
    "        self.fc3 = nn.Linear(latent_dim, 64*7*7)\n",
    "        \n",
    "        self.decoder = nn.Sequential(nn.ConvTranspose2d(64*7*7,32,kernel_size=5,stride=2),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.ConvTranspose2d(32,16,kernel_size=6,stride=2),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.ConvTranspose2d(16,1,kernel_size=2,stride=2),\n",
    "                                    nn.Sigmoid()\n",
    "                                    )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        # Flatten encoder output layer\n",
    "        h = self.encoder(x).view(x.size(0),-1)\n",
    "        mu = self.fc1(h)\n",
    "        logvar = self.fc2(h)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        #print('z shape =',z.shape)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        z = mu + (eps * std) # sampling as if coming from the input space\n",
    "        return z\n",
    "        \n",
    "    def decode(self, z):\n",
    "        z = self.fc3(z).view(z.size(0),64*7*7,1,1)\n",
    "        z = self.decoder(z)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "        z = self.decode(z)\n",
    "        return z, mu, logvar\n",
    "\n",
    "\n",
    "model = VAE(latent_dim).to(device)\n",
    "criterion = nn.BCELoss(reduction='sum')\n",
    "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total number of parameters is: {}\".format(params))\n",
    "print(model)\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeSX6RZhJtMB"
   },
   "source": [
    "--- \n",
    "\n",
    "## Part 1.1b: Training the Model (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JN-Pc0mvq-7_"
   },
   "source": [
    "### Defining a Loss\n",
    "Recall the Beta VAE loss, with an encoder $q$ and decoder $p$:\n",
    "$$ \\mathcal{L}=\\mathbb{E}_{q_\\phi(z \\mid X)}[\\log p_\\theta(X \\mid z)]-\\beta D_{K L}[q_\\phi(z \\mid X) \\| p_\\theta(z)]$$\n",
    "\n",
    "In order to implement this loss you will need to think carefully about your model's outputs and the choice of prior.\n",
    "\n",
    "There are multiple accepted solutions. Explain your design choices based on the assumptions you make regarding the distribution of your data.\n",
    "\n",
    "* Hint: this refers to the log likelihood as mentioned in the tutorial. Make sure these assumptions reflect on the values of your input data, i.e. depending on your choice you might need to do a simple preprocessing step.\n",
    "\n",
    "* You are encouraged to experiment with the weighting coefficient $\\beta$ and observe how it affects your training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "F6CeeS9CJtMC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/1] Its(0/1875) Loss(total): 120660.477 Loss(BCE): 73580.500 Loss(KL): 47079.977\n",
      "Epoch[1/1] Its(100/1875) Loss(total): 113934.266 Loss(BCE): 70393.273 Loss(KL): 43540.996\n",
      "Epoch[1/1] Its(200/1875) Loss(total): 115174.734 Loss(BCE): 75142.727 Loss(KL): 40032.008\n",
      "Epoch[1/1] Its(300/1875) Loss(total): 116069.664 Loss(BCE): 79338.703 Loss(KL): 36730.961\n",
      "Epoch[1/1] Its(400/1875) Loss(total): 105341.953 Loss(BCE): 71331.211 Loss(KL): 34010.738\n",
      "Epoch[1/1] Its(500/1875) Loss(total): 109336.711 Loss(BCE): 77668.242 Loss(KL): 31668.469\n",
      "Epoch[1/1] Its(600/1875) Loss(total): 104467.883 Loss(BCE): 74658.414 Loss(KL): 29809.469\n",
      "Epoch[1/1] Its(700/1875) Loss(total): 103181.211 Loss(BCE): 75081.609 Loss(KL): 28099.604\n",
      "Epoch[1/1] Its(800/1875) Loss(total): 102688.578 Loss(BCE): 76005.867 Loss(KL): 26682.709\n",
      "Epoch[1/1] Its(900/1875) Loss(total): 93191.547 Loss(BCE): 67997.562 Loss(KL): 25193.988\n",
      "Epoch[1/1] Its(1000/1875) Loss(total): 100860.344 Loss(BCE): 77068.422 Loss(KL): 23791.922\n",
      "Epoch[1/1] Its(1100/1875) Loss(total): 104161.500 Loss(BCE): 81381.477 Loss(KL): 22780.023\n",
      "Epoch[1/1] Its(1200/1875) Loss(total): 102995.969 Loss(BCE): 81222.227 Loss(KL): 21773.746\n",
      "Epoch[1/1] Its(1300/1875) Loss(total): 94332.750 Loss(BCE): 73247.797 Loss(KL): 21084.955\n",
      "Epoch[1/1] Its(1400/1875) Loss(total): 93014.469 Loss(BCE): 72444.688 Loss(KL): 20569.781\n",
      "Epoch[1/1] Its(1500/1875) Loss(total): 93169.289 Loss(BCE): 73067.398 Loss(KL): 20101.889\n",
      "Epoch[1/1] Its(1600/1875) Loss(total): 95514.484 Loss(BCE): 75903.531 Loss(KL): 19610.949\n",
      "Epoch[1/1] Its(1700/1875) Loss(total): 86423.734 Loss(BCE): 67188.609 Loss(KL): 19235.123\n",
      "Epoch[1/1] Its(1800/1875) Loss(total): 94655.867 Loss(BCE): 75799.781 Loss(KL): 18856.088\n"
     ]
    }
   ],
   "source": [
    "# *CODE FOR PART 1.1b IN THIS CELL*\n",
    "\n",
    "def loss_function_VAE(recon_x, x, mu, logvar, beta):\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        \n",
    "        eps = 1e-10\n",
    "        BCE = F.binary_cross_entropy(recon_x+eps, x+eps, reduction='sum')\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) * beta\n",
    "        \n",
    "        return BCE + KLD, BCE, KLD\n",
    "    \n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        ####################################################################### \n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):     \n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        #data = None # One batch\n",
    "        \n",
    "        for idx, (data,_) in enumerate(loader_train):\n",
    "            data = data.to(device)\n",
    "            # Forward pass\n",
    "            recon_x, mu, logvar = model(data)\n",
    "            # Calculate loss\n",
    "            loss, BCE, KLD = loss_function_VAE(recon_x, data, mu, logvar, beta=1)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if idx % 100 == 0:\n",
    "                _log = \"Epoch[{}/{}] Its({}/{}) Loss(total): {:.3f} Loss(BCE): {:.3f} Loss(KL): {:.3f}\".format(epoch+1,num_epochs,idx,len(loader_train),loss,BCE,KLD)\n",
    "                print(_log)\n",
    "            \n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        ####################################################################### \n",
    "        \n",
    "        # save the model\n",
    "        if epoch == num_epochs - 1:\n",
    "            with torch.no_grad():\n",
    "                torch.jit.save(torch.jit.trace(model, (data), check_trace=False),'/Users/jakecunningham/Documents/Imperial/Deep_Learning/Coursework/CW2/CW_VAE/VAE_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vF6B26_oJtMF"
   },
   "source": [
    "### Loss Explanation\n",
    "Explain your choice of loss and how this relates to:\n",
    "\n",
    "* The VAE Prior\n",
    "* The output data domain\n",
    "* Disentanglement in the latent space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUqWwUvlrYnH"
   },
   "outputs": [],
   "source": [
    "# Any code for your explanation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhjE07mrB7Zs"
   },
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ez5nlMi1JtMF"
   },
   "source": [
    "<h2>Part 1.2 (9 points)</h2>\n",
    "\n",
    "a. Plot your loss curves\n",
    "\n",
    "b. Show reconstructions and samples\n",
    "\n",
    "c. Discuss your results from parts (a) and (b)\n",
    "\n",
    "## Part 1.2a: Loss Curves (3 Points)\n",
    "Plot your loss curves (6 in total, 3 for the training set and 3 for the test set): total loss, reconstruction log likelihood loss, KL loss (x-axis: epochs, y-axis: loss). If you experimented with different values of $\\beta$, you may wish to display multiple plots (worth 1 point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AADYspqtJtMG"
   },
   "outputs": [],
   "source": [
    "# *CODE FOR PART 1.2a IN THIS CELL*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7wp4MzqsjjZ"
   },
   "source": [
    "## Part 1.2b: Samples and Reconstructions (6 Points)\n",
    "Visualize a subset of the images of the test set and their reconstructions **as well as** a few generated samples. Most of the code for this part is provided. You only need to call the forward pass of the model for the given inputs (might vary depending on your implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wu9CWtqoJtMK"
   },
   "outputs": [],
   "source": [
    "# *CODE FOR PART 1.2b IN THIS CELL*\n",
    "\n",
    "# load the model\n",
    "print('Input images')\n",
    "print('-'*50)\n",
    "\n",
    "sample_inputs, _ = next(iter(loader_test))\n",
    "fixed_input = sample_inputs[0:32, :, :, :]\n",
    "# visualize the original images of the last batch of the test set\n",
    "img = make_grid(denorm(fixed_input), nrow=8, padding=2, normalize=False,\n",
    "                range=None, scale_each=False, pad_value=0)\n",
    "plt.figure()\n",
    "show(img)\n",
    "\n",
    "print('Reconstructed images')\n",
    "print('-'*50)\n",
    "with torch.no_grad():\n",
    "    # visualize the reconstructed images of the last batch of test set\n",
    "    \n",
    "    #######################################################################\n",
    "    #                       ** START OF YOUR CODE **\n",
    "    #######################################################################\n",
    "    recon_batch = \n",
    "    #######################################################################\n",
    "    #                       ** END OF YOUR CODE **\n",
    "    ####################################################################### \n",
    "    \n",
    "    recon_batch = recon_batch.cpu()\n",
    "    recon_batch = make_grid(denorm(recon_batch), nrow=8, padding=2, normalize=False,\n",
    "                            range=None, scale_each=False, pad_value=0)\n",
    "    plt.figure()\n",
    "    show(recon_batch)\n",
    "\n",
    "print('Generated Images')  \n",
    "print('-'*50)\n",
    "model.eval()\n",
    "n_samples = 256\n",
    "z = torch.randn(n_samples,latent_dim).to(device)\n",
    "with torch.no_grad():\n",
    "    #######################################################################\n",
    "    #                       ** START OF YOUR CODE **\n",
    "    #######################################################################\n",
    "    samples =\n",
    "    #######################################################################\n",
    "    #                       ** END OF YOUR CODE **\n",
    "    ####################################################################### \n",
    "    \n",
    "    samples = samples.cpu()\n",
    "    samples = make_grid(denorm(samples), nrow=16, padding=2, normalize=False,\n",
    "                            range=None, scale_each=False, pad_value=0)\n",
    "    plt.figure(figsize = (8,8))\n",
    "    show(samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfZJRgj1rudZ"
   },
   "source": [
    "### Discussion\n",
    "Provide a brief analysis of your loss curves and reconstructions: \n",
    "* What do you observe in the behaviour of the log-likelihood loss and the KL loss (increasing/decreasing)?\n",
    "* Can you intuitively explain if this behaviour is desirable? Have you observed posterior collapse during traing (i.e. when the KL is too small during the early stages of training)? \n",
    "    * If yes, how did you mitigate it? How did this phenomenon reflect on your output samples?\n",
    "    * If no, why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vy4KKp2UJtMJ"
   },
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTprojS7sLP8"
   },
   "source": [
    "---\n",
    "<h2> Part 1.3 (11 points) <h2/>\n",
    "\n",
    "Qualitative analysis of the learned representations\n",
    "\n",
    "In this question you are asked to qualitatively assess the representations that your model has learned. In particular:\n",
    "\n",
    "a. Dimensionality Reduction of learned embeddings\n",
    "\n",
    "b. Interpolating in the latent space\n",
    "\n",
    "## Part 1.3a: T-SNE on Embeddings (7 Points)\n",
    "Extract the latent representations of the test set and visualize them using [T-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)  [(see implementation)](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). \n",
    "\n",
    "We've provided a function to visualize a subset of the data, but you are encouraged to also produce a matplotlib plot (please use different colours for each digit class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xl4xZOg7s0ke",
    "outputId": "4af471ef-cb1c-4dee-d80e-0ef20981f5d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing TSNE\n"
     ]
    }
   ],
   "source": [
    "# *CODE FOR PART 1.3a IN THIS CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-M_EI2ZnnXHZ"
   },
   "outputs": [],
   "source": [
    "# Interactive Visualization - Code Provided\n",
    "test_dataloader = DataLoader(test_dat, 10000, shuffle=False)\n",
    "\"\"\" Inputs to the function are\n",
    "        z_embedded - X, Y positions for every point in test_dataloader\n",
    "        test_dataloader - dataloader with batchsize set to 10000\n",
    "        num_points - number of points plotted (will slow down with >1k)\n",
    "\"\"\"\n",
    "plot_tsne(z_embedded, test_dataloader, num_points=1000, darkmode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xvQvtlDzIB3M"
   },
   "outputs": [],
   "source": [
    "# Custom Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HiAHb0ztTW8"
   },
   "source": [
    "### Discussion\n",
    "What do you observe? Discuss the structure of the visualized representations. \n",
    "* What do you observe? What role do the KL loss term and $\\beta$ have, if any, in what you observe (multiple matplotlib plots may be desirable here)?\n",
    "    * Consider Outliers\n",
    "    * Counsider Boundaries\n",
    "    * Consider Clusters\n",
    "* Is T-SNE reliable? What happens if you change the parameters (don't worry about being particularly thorough). [This link](https://distill.pub/2016/misread-tsne/) may be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0_3QlEYteYk"
   },
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCtbTLv4thEH"
   },
   "source": [
    "## Part 1.3b: Interpolating in $z$ (4 Points)\n",
    "Perform a linear interpolation in the latent space of the autoencoder by choosing any two digits from the test set. What do you observe regarding the transition from on digit to the other?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVk7GUIxtgiF"
   },
   "outputs": [],
   "source": [
    "# CODE FOR PART 1.3b IN THIS CELL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdk6yyrittNx"
   },
   "source": [
    "### Discussion\n",
    "What did you observe in the interpolation? Is this what you expected?\n",
    "* Can you relate the interpolation to your T-SNE visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZF2jUHWHtt3V"
   },
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EG68ntJ2qfIC"
   },
   "source": [
    "# Part 2 - Deep Convolutional GAN\n",
    "\n",
    "In this task, your main objective is to train a DCGAN (https://arxiv.org/abs/1511.06434) on the CIFAR-10 dataset. You should experiment with different architectures, tricks for stability in training (such as using different activation functions, batch normalization, different values for the hyper-parameters, etc.). In the end, you should provide us with: \n",
    "\n",
    "- your best trained model (which we will be able to run), \n",
    "- some generations for the fixed latent vectors $\\mathbf{z}\\sim \\mathcal{N}\\left(\\mathbf{0}, \\mathbf{I}\\right)$ we have provided you with (train for a number of epochs and make sure there is no mode collapse), \n",
    "- plots with the losses for the discriminator $D$ and the generator $G$ as the training progresses and explain whether your produced plots are theoretically sensible and why this is (or not) the case. \n",
    "- a discussion on whether you noticed any mode collapse, where this behaviour may be attributed to, and explanations of what you did in order to cope with mode collapse. \n",
    "\n",
    "## Part 2.1 (30 points)\n",
    "**Your Task**: \n",
    "\n",
    "a. Implement the DCGAN architecture. \n",
    "\n",
    "b. Define a loss and implement the Training Loop\n",
    "\n",
    "c. Visualize images sampled from your best model's generator (\"Extension\" Assessed on quality)\n",
    "\n",
    "d. Discuss the experimentations which led to your final architecture. You can plot losses or generated results by other architectures that you tested to back your arguments (but this is not necessary to get full marks).\n",
    "\n",
    "\n",
    "_Clarification: You should not be worrying too much about getting an \"optimal\" performance on your trained GAN. We want you to demonstrate to us that you experimented with different types of DCGAN variations, report what difficulties transpired throughout the training process, etc. In other words, if we see that you provided us with a running implementation, that you detail different experimentations that you did before providing us with your best one, and that you have grapsed the concepts, you can still get good marks. The attached model does not have to be perfect, and the extension marks for performance are only worth 10 points._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uFEt7wGXP_aE",
    "outputId": "a93f75c8-8a22-4386-8d0d-ddb799565d2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f66859a1bb8>"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def denorm(x, channels=None, w=None ,h=None, resize = False):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    if resize:\n",
    "        if channels is None or w is None or h is None:\n",
    "            print('Number of channels, width and height must be provided for resize.')\n",
    "        x = x.view(x.size(0), channels, w, h)\n",
    "    return x\n",
    "\n",
    "def show(img):\n",
    "    npimg = img.cpu().numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "\n",
    "if not os.path.exists('/content/drive/MyDrive/icl_dl_cw2/CW_GAN'):\n",
    "    os.makedirs('/content/drive/MyDrive/icl_dl_cw2/CW_GAN')\n",
    "\n",
    "GPU = True # Choose whether to use GPU\n",
    "if GPU:\n",
    "    device = torch.device(\"cuda\"  if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f'Using {device}')\n",
    "\n",
    "# We set a random seed to ensure that your results are reproducible.\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VosOpcpfGvWO"
   },
   "source": [
    "### Part 2.1a: Implement DCGAN (8 Points)\n",
    "Fill in the missing parts in the cells below in order to complete the Generator and Discriminator classes. You will need to define:\n",
    "\n",
    "- The hyperparameters\n",
    "- The constructors\n",
    "- `decode`\n",
    "- `discriminator`\n",
    "\n",
    "Recomendations for experimentation:\n",
    "- use the architecture that you implemented for the Autoencoder of Part 1 (encoder as discriminator, decoder as generator).\n",
    "- use the architecture desribed in the DCGAN paper (https://arxiv.org/abs/1511.06434).\n",
    "\n",
    "Some general reccomendations:\n",
    "- add several convolutional layers (3-4).\n",
    "- accelerate training with batch normalization after every convolutional layer.\n",
    "- use the appropriate activation functions. \n",
    "- Generator module: the upsampling can be done with various methods, such as nearest neighbor upsampling (`torch.nn.Upsample`) or transposed convolutions(`torch.nn.ConvTranspose2d`). \n",
    "- Discriminator module: Experiment with batch normalization (`torch.nn.BatchNorm2d`) and leaky relu (`torch.nn.LeakyReLu`) units after each convolutional layer.\n",
    "\n",
    "Try to follow the common practices for CNNs (e.g small receptive fields, max pooling, RELU activations), in order to narrow down your possible choices.\n",
    "\n",
    "<font color=\"red\">**Your model should not have more than 25 Million Parameters**</font>\n",
    "\n",
    "The number of epochs that will be needed in order to train the network will vary depending on your choices. As an advice, we recommend that while experimenting you should allow around 20 epochs and if the loss doesn't sufficiently drop, restart the training with a more powerful architecture. You don't need to train the network to an extreme if you don't have the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOi_Q_jleQJq"
   },
   "source": [
    "#### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__ENlW2aeQJr"
   },
   "outputs": [],
   "source": [
    "batch_size =   # change that\n",
    "\n",
    "transform = transforms.Compose([\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),),                        \n",
    "])\n",
    "\n",
    "data_dir = './datasets'\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(data_dir, train=True, download=True, transform=transform)\n",
    "cifar10_test = datasets.CIFAR10(data_dir, train=False, download=True, transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=batch_size)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TmrRudFRhOB"
   },
   "source": [
    "We'll visualize a subset of the test set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CY2ka775Rfxm"
   },
   "outputs": [],
   "source": [
    "samples, _ = next(iter(loader_test))\n",
    "\n",
    "samples = samples.cpu()\n",
    "samples = make_grid(denorm(samples), nrow=8, padding=2, normalize=False,\n",
    "                        range=None, scale_each=False, pad_value=0)\n",
    "plt.figure(figsize = (15,15))\n",
    "plt.axis('off')\n",
    "show(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSkveP0mZ-Pd"
   },
   "source": [
    "#### Model Definition\n",
    "Define hyperparameters and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FBoxMTihZ-Pd"
   },
   "outputs": [],
   "source": [
    "# *CODE FOR PART 2.1 IN THIS CELL*\n",
    "\n",
    "# Choose the number of epochs, the learning rate\n",
    "# and the size of the Generator's input noise vetor.\n",
    "\n",
    "num_epochs = \n",
    "learning_rate =\n",
    "latent_vector_size = \n",
    "\n",
    "# Other hyperparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ATTUAhCDZ-Pg"
   },
   "outputs": [],
   "source": [
    "# *CODE FOR PART 2.1 IN THIS CELL*\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        ####################################################################### \n",
    "\n",
    "\n",
    "    def forward(self, z, label):\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "       \n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        return out\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "     \n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        ####################################################################### \n",
    "        \n",
    "    def forward(self, x, label):\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "     \n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        ####################################################################### \n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8YDyYf8Z-Pi"
   },
   "source": [
    "<h2> Initialize Model and print number of parameters </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xh3NpfD_Z-Pj"
   },
   "source": [
    "You can use method `weights_init` to initialize the weights of the Generator and Discriminator networks. Otherwise, implement your own initialization, or do not use at all. You will not be penalized for not using initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JAVpgpmUZ-Pk"
   },
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ew-OdvNJZ-Pm"
   },
   "outputs": [],
   "source": [
    "use_weights_init = True\n",
    "\n",
    "model_G = Generator().to(device)\n",
    "if use_weights_init:\n",
    "    model_G.apply(weights_init)\n",
    "params_G = sum(p.numel() for p in model_G.parameters() if p.requires_grad)\n",
    "print(\"Total number of parameters in Generator is: {}\".format(params_G))\n",
    "print(model_G)\n",
    "print('\\n')\n",
    "\n",
    "model_D = Discriminator().to(device)\n",
    "if use_weights_init:\n",
    "    model_D.apply(weights_init)\n",
    "params_D = sum(p.numel() for p in model_D.parameters() if p.requires_grad)\n",
    "print(\"Total number of parameters in Discriminator is: {}\".format(params_D))\n",
    "print(model_D)\n",
    "print('\\n')\n",
    "\n",
    "print(\"Total number of parameters is: {}\".format(params_G + params_D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "by_TNUPXJamb"
   },
   "source": [
    "### Part 2.1b: Training the Model (12 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00wgs1VNZ-Pp"
   },
   "source": [
    "#### Defining a Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPlxaL_cZ-Pq"
   },
   "outputs": [],
   "source": [
    "def loss_function(out, label):\n",
    "    loss = \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrgmhlSXZ-Ps"
   },
   "source": [
    "<h3>Choose and initialize optimizers</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pFM8iI24Z-Pt"
   },
   "outputs": [],
   "source": [
    "# setup optimizer\n",
    "# You are free to add a scheduler or change the optimizer if you want. We chose one for you for simplicity.\n",
    "beta1 = 0.5\n",
    "optimizerD = torch.optim.Adam(model_D.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n",
    "optimizerG = torch.optim.Adam(model_G.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZ311RPlZ-Pv"
   },
   "source": [
    "<h3> Define fixed input vectors to monitor training and mode collapse. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGB_9A1UZ-Pw"
   },
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(batch_size, latent_vector_size, 1, 1, device=device)\n",
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gD9S_3yZZ-Py"
   },
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5vD--X6Z-Pz"
   },
   "outputs": [],
   "source": [
    "train_losses_G = []\n",
    "train_losses_D = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(loader_train, 0):\n",
    "        train_loss_D = 0\n",
    "        train_loss_G = 0\n",
    "        \n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################device\n",
    "        # train with real\n",
    "\n",
    "\n",
    "        # train with fake\n",
    "\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        \n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "              % (epoch, num_epochs, i, len(loader_train),\n",
    "                 errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "    if epoch == 0:\n",
    "        save_image(denorm(real_cpu.cpu()).float(), '/content/drive/MyDrive/icl_dl_cw2/CW_GAN/real_samples.png')\n",
    "    with torch.no_grad():\n",
    "        fake = model_G(fixed_noise)\n",
    "        save_image(denorm(fake.cpu()).float(), '/content/drive/MyDrive/icl_dl_cw2/CW_GAN/fake_samples_epoch_%03d.png' % epoch)\n",
    "    train_losses_D.append(train_loss_D / len(loader_train))\n",
    "    train_losses_G.append(train_loss_G / len(loader_train))\n",
    "   \n",
    "    \n",
    "# save  models \n",
    "# if your discriminator/generator are conditional you'll want to change the inputs here\n",
    "torch.jit.save(torch.jit.trace(model_G, (fixed_noise)), '/content/drive/MyDrive/icl_dl_cw2/CW_GAN/GAN_G_model.pth')\n",
    "torch.jit.save(torch.jit.trace(model_D, (fake)), '/content/drive/MyDrive/icl_dl_cw2/CW_GAN/GAN_D_model.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOjxGURDINm7"
   },
   "source": [
    "## Part 2.1c: Results (10 Points)\n",
    "This part is fairly open-ended, but not worth too much so do not go crazy. The table below shows examples of what are considered good samples. Level 3 and above will get you 10/10 points, level 2 will roughly get you 5/10 points and level 1 and below will get you 0/10 points.\n",
    "\n",
    "<table><tr>\n",
    "<td> \n",
    "  <p align=\"center\" style=\"padding: 10px\">\n",
    "    <img alt=\"Forwarding\" src=\"https://drive.google.com/uc?id=1wQ2f10-A1Vs7k0LMfBPPyYTsPlkBF9QE\" width=\"%30\">\n",
    "    <br>\n",
    "    <em style=\"color: grey\">Level 1</em>\n",
    "  </p> \n",
    "</td>\n",
    "<td> \n",
    "  <p align=\"center\">\n",
    "    <img alt=\"Routing\" src=\"https://drive.google.com/uc?id=1wlDhX4hROET4s8Ndxn8nhj_0RLM2rnuG\" width=\"%30\">\n",
    "    <br>\n",
    "    <em style=\"color: grey\">Level 2</em>\n",
    "  </p> \n",
    "</td>\n",
    "<td> \n",
    "  <p align=\"center\">\n",
    "    <img alt=\"Routing\" src=\"https://drive.google.com/uc?id=1w9VrgfJLCRaTPhwoFVYdYhtCeaQmFHGb\" width=\"%30\">\n",
    "    <br>\n",
    "    <em style=\"color: grey\">Level 3</em>\n",
    "  </p> \n",
    "</td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IQIKTdPZ-P5"
   },
   "source": [
    "### Generator samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIHi0HrJZ-P8"
   },
   "outputs": [],
   "source": [
    "input_noise = torch.randn(100, latent_vector_size, 1, 1, device=device)\n",
    "with torch.no_grad():\n",
    "    # visualize the generated images\n",
    "    generated = model_G(input_noise).cpu()\n",
    "    generated = make_grid(denorm(generated)[:100], nrow=10, padding=2, normalize=False, \n",
    "                        range=None, scale_each=False, pad_value=0)\n",
    "    plt.figure(figsize=(15,15))\n",
    "    save_image(generated,'/content/drive/MyDrive/icl_dl_cw2/CW_GAN/Teaching30final.png')\n",
    "    show(generated) # note these are now class conditional images columns rep classes 1-10\n",
    "\n",
    "it = iter(loader_test)\n",
    "sample_inputs, _ = next(it)\n",
    "fixed_input = sample_inputs[0:64, :, :, :]\n",
    "# visualize the original images of the last batch of the test set for comparison\n",
    "img = make_grid(denorm(fixed_input), nrow=8, padding=2, normalize=False,\n",
    "                range=None, scale_each=False, pad_value=0)\n",
    "plt.figure(figsize=(15,15))\n",
    "show(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tRtYjH_LbQ0"
   },
   "source": [
    "## Part 2.1d: Engineering Choices (10 Points)\n",
    "\n",
    "Discuss the process you took to arrive at your final architecture. This should include:\n",
    "\n",
    "* Which empirically useful methods did you utilize\n",
    "* What didn't work, what worked and what mattered most\n",
    "* Are there any tricks you came across in the literature etc. which you suspect would be helpful here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnFqwePhXeZ4"
   },
   "source": [
    "**Your Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DC6ndLP5Z-P-"
   },
   "source": [
    "## Part 2.2: Understanding GAN Training (5 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zz6oy7ixZ-P_"
   },
   "source": [
    "### Loss Curves\n",
    "**Your task:**\n",
    "\n",
    "\n",
    "Plot the losses curves for the discriminator $D$ and the generator $G$ as the training progresses and explain whether the produced curves are theoretically sensible and why this is (or not) the case (x-axis: epochs, y-axis: loss).\n",
    "\n",
    "Make sure that the version of the notebook you deliver includes these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kxrUDHfBZ-QA"
   },
   "outputs": [],
   "source": [
    "# ANSWER FOR PART 2.2 IN THIS CELL*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZCUIHKkS0kF"
   },
   "source": [
    "### Discussion\n",
    "\n",
    "Do your loss curves look sensible? What would you expect to see and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYYOnd6YBN3k"
   },
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_WLkdSNZ-QE"
   },
   "source": [
    "## Part 2.3: Understanding Mode Collapse (5 points) \n",
    "**Your task:** \n",
    "\n",
    "Based on the images created by your generator using the `fixed_noise` vector during training, provide a discussion on whether you noticed any mode collapse, what this behaviour may be attributed to, and explain what you did in order to cope with mode collapse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HiPei-I0FCGM"
   },
   "outputs": [],
   "source": [
    "# Any additional code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpZuxPYUFedE"
   },
   "source": [
    "### Discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SynUx_QV7olI"
   },
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cud7gZw2M-0U"
   },
   "source": [
    "\n",
    "\n",
    "# TA Test Cell\n",
    "TAs will run this cell to ensure that your results are reproducible, and that your models have been defined suitably. \n",
    "\n",
    "<font color=\"blue\"> <b> Please provide the input and output transformations required to make your VAE and GANs work. If your GAN generator requires more than just noise as input, also specify this below (there are two marked cells for you to inspect) </b></font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JesvipjEFbGx"
   },
   "outputs": [],
   "source": [
    "# If you want to run these tests yourself, change directory:\n",
    "# !cd /content/drive/MyDrive/icl_dl_cw2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLlCRIn6m9ZS"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FUmBbya2nQh9",
    "outputId": "2ae37d0c-d6ed-4eaf-c548-37c554bb4735"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb1d90f4bd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do not remove anything here\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, sampler\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "show = lambda img: plt.imshow(np.transpose(img.cpu().numpy(), (1,2,0)))\n",
    "\n",
    "device = torch.device(\"cuda\"  if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Do not change this cell!\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IFfBZsnTzQIU"
   },
   "outputs": [],
   "source": [
    "############# CHANGE THESE (COPY AND PASTE FROM YOUR OWN CODE) #############\n",
    "vae_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def vae_denorm(x):\n",
    "    return x\n",
    "\n",
    "def gan_denorm(x):\n",
    "    return x\n",
    "\n",
    "gan_latent_size = \n",
    "\n",
    "# If your generator requires something other than noise as input, please specify\n",
    "# two cells down from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5t0CVMCFyxgU"
   },
   "outputs": [],
   "source": [
    "# Load VAE Dataset\n",
    "test_dat = datasets.MNIST(\"./data/\", train=False, transform=vae_transform, \n",
    "                          download=True)\n",
    "vae_loader_test = DataLoader(test_dat, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HTNsHqK-mh6U"
   },
   "outputs": [],
   "source": [
    "############# MODIFY IF NEEDED #############\n",
    "vae_input, _ = next(iter(vae_loader_test))\n",
    "\n",
    "# If your generator is conditional, then please modify this input suitably\n",
    "input_noise = torch.randn(100, gan_latent_size, 1, 1, device=device)\n",
    "gan_input = [input_noise] # In case you want to provide a tuple, we wrap ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EIg06rOqo3XA"
   },
   "outputs": [],
   "source": [
    "# VAE Tests\n",
    "# TAs will change these paths as you will have provided the model files manually\n",
    "\"\"\"To TAs, you should have been creating a folder with the student uid\n",
    "   And the .ipynb + models in the root. Then that path is './VAE_model.pth' etc.\n",
    "\"\"\"\n",
    "vae = model_G = torch.jit.load('./CW_VAE/VAE_model.pth')\n",
    "vae.eval()\n",
    "\n",
    "# Check if VAE is convolutional\n",
    "\n",
    "for module in vae.children():\n",
    "    for layer in module.children():\n",
    "        if \"Conv2d\" in layer.original_name:\n",
    "            print(\"Used Convs\")\n",
    "            break\n",
    "\n",
    "vae_in = make_grid(denorm(vae_input), nrow=8, padding=2, normalize=False,\n",
    "                range=None, scale_each=False, pad_value=0)\n",
    "plt.figure()\n",
    "plt.axis('off')\n",
    "show(vae_in)\n",
    "\n",
    "vae_test = vae(vae_input.to(device))[0].detach()\n",
    "vae_reco = make_grid(denorm(vae_test), nrow=8, padding=2, normalize=False,\n",
    "                range=None, scale_each=False, pad_value=0)\n",
    "plt.figure()\n",
    "plt.axis('off')\n",
    "show(vae_reco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HoJLGFZSo7ON"
   },
   "outputs": [],
   "source": [
    "# GAN Tests\n",
    "model_G = torch.jit.load('./CW_GAN/GAN_G_model.pth')\n",
    "model_D = torch.jit.load('./CW_GAN/GAN_D_model.pth')\n",
    "[model.eval() for model in (model_G, model_D)]  \n",
    "\n",
    "# Check that GAN doesn't have too many parameters\n",
    "num_param = sum(p.numel() for p in [*model_G.parameters(),*model_D.parameters()])\n",
    "\n",
    "print(f\"Number of Parameters is {num_param} which is\", \"ok\" if num_param<25E+6 else \"not ok\")\n",
    "\n",
    "# visualize the generated images\n",
    "generated = model_G(*gan_input).cpu()\n",
    "generated = make_grid(gan_denorm(generated)[:100].detach(), nrow=10, padding=2, normalize=False, \n",
    "                    range=None, scale_each=False, pad_value=0)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.axis('off')\n",
    "show(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9H91RxRuQm1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dl_cw2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
